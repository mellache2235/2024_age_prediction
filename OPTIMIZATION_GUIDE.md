# Brain-Behavior Optimization Guide

## ğŸ¯ Goal
Maximize Spearman correlations between brain features (IG scores) and behavioral measures using comprehensive hyperparameter optimization.

---

## ğŸ“‹ Step-by-Step Workflow

### Step 1: Run Optimization

Choose the appropriate script for your cohort:

#### Dedicated Scripts (Recommended):
```bash
# Stanford ASD (SRS measures)
python scripts/run_stanford_asd_brain_behavior_optimized.py

# ABIDE ASD (ADOS measures)
python scripts/run_abide_asd_brain_behavior_optimized.py

# NKI (CAARS/Conners measures)
python scripts/run_nki_brain_behavior_optimized.py
```

#### Universal Script (ADHD cohorts):
```bash
# ADHD200 or CMI-HBN
python scripts/run_all_cohorts_brain_behavior_optimized.py --cohort adhd200_td
python scripts/run_all_cohorts_brain_behavior_optimized.py --cohort adhd200_adhd
python scripts/run_all_cohorts_brain_behavior_optimized.py --cohort cmihbn_td
python scripts/run_all_cohorts_brain_behavior_optimized.py --cohort cmihbn_adhd
```

**Test mode** (2 measures, ~5 min):
```bash
python scripts/run_stanford_asd_brain_behavior_optimized.py --max-measures 2
```

**Runtime**: 30-60 minutes for full analysis

---

### Step 2: What Happens During Optimization

For each behavioral measure, the script:

1. **Loads data** (IG features + behavioral scores)
2. **Removes outliers** (IQR Ã— 3 method)
3. **Tests ~100-200 configurations**:
   - PCA + Regression (5-50 components Ã— 4 models Ã— 7 alphas)
   - PLS Regression (3-30 components)
   - Feature Selection + Regression (4 feature counts Ã— 2 methods)
   - Direct Regularized Regression (7 alphas)
4. **Uses 5-fold cross-validation** for each config
5. **Selects best configuration** (highest CV Spearman Ï)
6. **Refits best model on ALL data**
7. **Evaluates and saves results**

**Expected warnings**: `SpearmanRConstantInputWarning` (20-50 times) - NORMAL! Bad configs being tested and rejected.

---

### Step 3: Review Console Output

For each behavioral measure, you'll see:

```
====================================================================================================
  ANALYZING: social_awareness_tscore
====================================================================================================

  Valid subjects: 99
  
  Optimizing social_awareness_tscore...
    Best: FeatureSelection+Lasso (Ï=0.0964)  â† CV score

ğŸ“Š PREDICTION INTEGRITY CHECK:
================================================================================

Actual values:
  N = 99
  Mean = 65.23, Std = 12.45, Range = [42.00, 95.00]

Predicted values:
  Mean = 64.87, Std = 11.98, Range = [45.12, 88.34]

Metrics:
  Spearman Ï = 0.293    â† Final score (on all data)
  P-value = 0.0033      â† Significance
  RÂ² = 0.095
  MAE = 5.26

âœ… No major issues detected  â† GOOD! Use this result

Sample predictions (first 5):
    Actual  Predicted   Residual
     65.20      63.40       1.80
     72.10      74.30      -2.20
     58.90      60.10      -1.20
     81.50      79.20       2.30
     55.00      57.80      -2.80
================================================================================

âœ“ Saved: scatter_social_awareness_tscore_FeatureSelection_Lasso_k50_optimized.png
```

**Key things to check**:
- âœ… or âŒ in integrity check
- Spearman Ï and p-value
- Prediction range (should be reasonable, not [-1000, 2000])

---

### Step 4: Files Produced

For each cohort, in `/oak/.../brain_behavior/{cohort}_optimized/`:

#### Summary Files:
- **`optimization_summary.csv`** - Best configuration for each measure
  ```csv
  Measure,N_Subjects,Best_Strategy,Best_Model,CV_Spearman,Final_Spearman,Final_P_Value,Final_R2
  social_awareness_tscore,99,FeatureSelection+Lasso,Lasso,0.096,0.293,0.0033,0.095
  ```

#### Per-Measure Files:
- **`scatter_{measure}_{method}_{params}_optimized.png/tiff/ai`** - Scatter plot
  - Example: `scatter_social_awareness_tscore_FeatureSelection_Lasso_k50_optimized.png`
  - Shows: r = 0.293, p = 0.0033, model info
  
- **`predictions_{measure}_{method}.csv`** - Actual vs predicted values
  ```csv
  Actual,Predicted,Residual
  65.20,63.40,1.80
  72.10,74.30,-2.20
  ...
  ```

- **`optimization_results_{measure}.csv`** - All ~200 configurations tested
  ```csv
  strategy,model,n_components,alpha,mean_cv_spearman,std_cv_spearman
  FeatureSelection+Lasso,Lasso,,0.1,0.0964,0.0234
  PLS,PLS,15,,0.0893,0.0198
  ...
  ```

---

### Step 5: Identify Good Results

#### Option 1: Check Console Output
Look for:
- âœ… **"No major issues detected"** â†’ Use this result
- âŒ **"ISSUES DETECTED"** â†’ Don't use

#### Option 2: Use Summary Script
```bash
python scripts/create_optimization_summary_figure.py --cohort stanford_asd --min-rho 0.2
```

**Produces**:
- `{cohort}_optimization_summary_significant.csv` - Only good results
- `{cohort}_correlations_barplot.png` - Visual summary
- `{cohort}_optimization_summary_figure.png` - Multi-panel figure

**Automatically filters out**:
- Failed models (extreme RÂ²)
- Non-significant results (p > 0.05)
- Weak correlations (|Ï| < threshold)

---

## ğŸ“Š Interpreting Results

### âœ… Good Result
```
Measure: social_awareness_tscore
Spearman Ï: 0.293, P-value: 0.0033, RÂ²: 0.095
Best: FeatureSelection+Lasso (k=50 features, Î±=0.1)
âœ… No major issues detected
```

**What to do**: 
- âœ“ Use this result for publication
- âœ“ Use the scatter plot
- âœ“ Report: "Ï = 0.29, p < 0.01, using feature selection"

---

### âŒ Failed Result
```
Measure: srs_total_score_standard
Spearman Ï: -0.022, P-value: 0.827, RÂ²: -6484
Best: PLS (but still terrible)
ğŸš¨ ISSUES DETECTED: Extreme RÂ², High MAE
```

**What to do**:
- âœ— Don't use this result
- âœ— Don't include in publication
- âœ“ Note: "No significant brain-behavior correlation found for this measure"

---

## ğŸ” Optimization Details

### What Gets Tested (~200 configurations):

| Strategy | Configurations |
|----------|----------------|
| **PCA + Linear** | 10 PC values = 10 configs |
| **PCA + Ridge** | 10 PC Ã— 7 alphas = 70 configs |
| **PCA + Lasso** | 10 PC Ã— 7 alphas = 70 configs |
| **PCA + ElasticNet** | 10 PC Ã— 7 alphas = 70 configs |
| **PLS** | 10 component values = 10 configs |
| **Feature Selection + Ridge/Lasso** | 2 methods Ã— 4 K Ã— 2 models Ã— 4 alphas = 64 configs |
| **Direct Ridge/Lasso/ElasticNet** | 3 models Ã— 7 alphas = 21 configs |
| **Total** | ~315 configurations |

### How Selection Works:
- Each config evaluated with 5-fold cross-validation
- **Spearman correlation** is the metric being maximized
- Best config (highest CV Spearman) is selected
- Best model refitted on ALL data
- Final performance reported

---

## ğŸ“ˆ Expected Performance

### Baseline (Enhanced Scripts):
- Uses: PCA (80% variance) + LinearRegression
- Example NKI: Ï = 0.35 - 0.41

### Optimized (Should Achieve):
- Tests: All strategies above
- Expected: Ï = baseline to +30%
- Example NKI: Ï = 0.41 - 0.50

**If optimized â‰ˆ baseline**: LinearRegression was already optimal!  
**If optimized > baseline (+10-30%)**: PLS or other strategy worked better!

---

## ğŸš¨ Common Issues

### Issue: "Per-column arrays must be 1-dimensional"
**Cause**: Running old code on Oak  
**Solution**: Ensure latest code is synced/cloned to Oak

### Issue: Many SpearmanRConstantInputWarnings
**Status**: **NORMAL!** âœ… Part of testing ~200 configs  
**Action**: None needed - bad configs auto-rejected

### Issue: Poor performance (Ï much worse than baseline)
**Causes**:
1. No real brain-behavior relationship (expected for some measures)
2. Data mismatch (wrong subjects loaded)
3. Old code on Oak (sync issue)

**Check**: Integrity output should show reasonable prediction ranges

---

## ğŸ“š Summary Table

| Step | Command | Output | Time |
|------|---------|--------|------|
| **1. Run Optimization** | `python run_{cohort}_optimized.py` | Tests ~200 configs | 30-60 min |
| **2. Check Console** | (automatic) | Integrity checks shown | Real-time |
| **3. Review Results** | Check for âœ… vs âŒ | Identify good measures | 1 min |
| **4. Create Summary** | `create_optimization_summary_figure.py` | Filtered results only | 1 min |
| **5. Use in Paper** | Copy .tiff files | Publication figures | Done! |

---

## ğŸ¯ Quick Checklist

Before running:
- [ ] Scripts are synced to Oak (if running there)
- [ ] IG CSV file exists
- [ ] Behavioral data exists

After running:
- [ ] Check for "No major issues detected" âœ…
- [ ] Verify p-value < 0.05
- [ ] Verify |Ï| > 0.2 (or your threshold)
- [ ] Check prediction ranges are reasonable
- [ ] Only use measures that passed all checks

For publication:
- [ ] Use measures with âœ… in integrity check
- [ ] Report both CV and Final Spearman
- [ ] Include p-values
- [ ] Note optimization method in methods section

---

## ğŸ“ All Cohorts Quick Reference

```bash
# Stanford ASD
python scripts/run_stanford_asd_brain_behavior_optimized.py

# ABIDE ASD  
python scripts/run_abide_asd_brain_behavior_optimized.py

# NKI
python scripts/run_nki_brain_behavior_optimized.py

# ADHD200 TD/ADHD, CMI-HBN TD/ADHD
python scripts/run_all_cohorts_brain_behavior_optimized.py --cohort {cohort_name}
```

**All produce same outputs**: optimization_summary.csv, scatter plots, predictions CSV, etc.

---

**That's it!** One guide for everything. ğŸ‰

